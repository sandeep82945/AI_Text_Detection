{
    "abstractText": "With the huge success of GPT models in natural language processing, there is a growing interest in applying language modeling approaches to speech tasks. Currently, the dominant architecture in speech-to-speech translation (S2ST) remains the encoder-decoder paradigm, creating a need to investigate the impact of language modeling approaches in this area. In this study, we introduce PolyVoice, a language model-based framework designed for S2ST systems. Our framework comprises three decoder-only language models: a translation language model, a duration language model, and a speech synthesis language model. These language models employ different types of prompts to extract learned information effectively. By utilizing unsupervised semantic units, our framework can transfer semantic information across these models, making it applicable even to unwritten languages. We evaluate our system on Chinese \u2192 English and English \u2192 Spanish language pairs. Experimental results demonstrate that PolyVoice outperforms the state-of-the-art encoder-decoder model, producing voice-cloned speech with high translation and audio quality. Speech samples are available at https://polyvoice.github.io.",
    "authors": [],
    "id": "SP:de16eb626c48a63a31dcb48adf89fba4bbf43d0c",
    "references": [
        {
            "authors": [
                "Sweta Agrawal",
                "Antonios Anastasopoulos",
                "Luisa Bentivogli",
                "Ond\u0159ej Bojar",
                "Claudia Borg",
                "Marine Carpuat",
                "Roldano Cattoni",
                "Mauro Cettolo",
                "Mingda Chen",
                "William Chen"
            ],
            "title": "Findings of the iwslt 2023 evaluation campaign",
            "venue": "In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023),",
            "year": 2023
        },
        {
            "authors": [
                "Jason Baldridge"
            ],
            "title": "Verbmobil: Foundations of Speech-to-Speech Translation, by wolfgang wahlster (editor)",
            "venue": "springer,",
            "year": 2000
        },
        {
            "authors": [
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Damien Vincent",
                "Eugene Kharitonov",
                "Olivier Pietquin",
                "Matt Sharifi",
                "Dominik Roblek",
                "Olivier Teboul",
                "David Grangier",
                "Marco Tagliasacchi",
                "Neil Zeghidour"
            ],
            "title": "Audiolm: A language modeling approach to audio generation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yutian Chen",
                "Yannis Assael",
                "Brendan Shillingford",
                "David Budden",
                "Scott Reed",
                "Heiga Zen",
                "Quan Wang",
                "Luis C. Cobo",
                "Andrew Trask",
                "Ben Laurie",
                "Caglar Gulcehre",
                "A\u00e4ron van den Oord",
                "Oriol Vinyals",
                "Nando de Freitas"
            ],
            "title": "Sample efficient adaptive text-to-speech",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Yu-An Chung",
                "Yu Zhang",
                "Wei Han",
                "Chung-Cheng Chiu",
                "James Qin",
                "Ruoming Pang",
                "Yonghui Wu"
            ],
            "title": "W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2021
        },
        {
            "authors": [
                "Erica Cooper",
                "Cheng-I Lai",
                "Yusuke Yasuda",
                "Fuming Fang",
                "Xin Wang",
                "Nanxin Chen",
                "Junichi Yamagishi"
            ],
            "title": "Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Qianqian Dong",
                "Fengpeng Yue",
                "Tom Ko",
                "Mingxuan Wang",
                "Qibing Bai",
                "Yu Zhang"
            ],
            "title": "Leveraging pseudo-labeled data to improve direct speech-to-speech translation",
            "venue": "In Interspeech",
            "year": 2022
        },
        {
            "authors": [
                "Yassir Fathullah",
                "Chunyang Wu",
                "Egor Lakomkin",
                "Junteng Jia",
                "Yuan Shangguan",
                "Ke Li",
                "Jinxi Guo",
                "Wenhan Xiong",
                "Jay Mahadeokar",
                "Ozlem Kalinli",
                "Christian Fuegen",
                "Mike Seltzer"
            ],
            "title": "Prompting large language models with speech recognition abilities",
            "venue": "arXiv preprint arXiv:2307.11795,",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed"
            ],
            "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Ye Jia",
                "Yu Zhang",
                "Ron Weiss",
                "Quan Wang",
                "Jonathan Shen",
                "Fei Ren",
                "Patrick Nguyen",
                "Ruoming Pang",
                "Ignacio Lopez Moreno",
                "Yonghui Wu"
            ],
            "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ye Jia",
                "Ron J. Weiss",
                "Fadi Biadsy",
                "Wolfgang Macherey",
                "Melvin Johnson",
                "Zhifeng Chen",
                "Yonghui Wu"
            ],
            "title": "Direct speech-to-speech translation with a sequence-to-sequence model",
            "venue": "In Gernot Kubin and Zdravko Kacic (eds.), Interspeech 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Ye Jia",
                "Michelle Tadmor Ramanovich",
                "Tal Remez",
                "Roi Pomerantz"
            ],
            "title": "Translatotron 2: Highquality direct speech-to-speech translation with voice preservation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ye Jia",
                "Michelle Tadmor Ramanovich",
                "Quan Wang",
                "Heiga Zen"
            ],
            "title": "Cvss corpus and massively multilingual speech-to-speech translation",
            "venue": "In Proceedings of the Thirteenth Language Resources and Evaluation Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Kahn",
                "Morgane Rivi\u00e8re",
                "Weiyi Zheng",
                "Evgeny Kharitonov",
                "Qiantong Xu",
                "Pierre-Emmanuel Mazar\u00e9",
                "Julien Karadayi",
                "Vitaliy Liptchinsky",
                "Ronan Collobert",
                "Christian Fuegen",
                "Tatiana Likhomanenko",
                "Gabriel Synnaeve",
                "Armand Joulin",
                "Abdelrahman Mohamed",
                "Emmanuel Dupoux"
            ],
            "title": "Libri-light: A benchmark for ASR with limited or no supervision",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Takatomo Kano",
                "Sakriani Sakti",
                "Satoshi Nakamura"
            ],
            "title": "Transformer-based direct speech-to-speech translation with transcoder",
            "venue": "In IEEE Spoken Language Technology Workshop,",
            "year": 2021
        },
        {
            "authors": [
                "Alon Lavie",
                "Alex Waibel",
                "Lori S. Levin",
                "Michael Finke",
                "Donna Gates",
                "Marsal Gavald\u00e0",
                "Torsten Zeppenfeld",
                "Puming Zhan"
            ],
            "title": "Janus-iii: speech-to-speech translation in multiple languages",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP \u201997,",
            "year": 1997
        },
        {
            "authors": [
                "Ann Lee",
                "Peng-Jen Chen",
                "Changhan Wang",
                "Jiatao Gu",
                "Sravya Popuri",
                "Xutai Ma",
                "Adam Polyak",
                "Yossi Adi",
                "Qing He",
                "Yun Tang",
                "Juan Pino",
                "Wei-Ning Hsu"
            ],
            "title": "Direct speech-to-speech translation with discrete units",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Ann Lee",
                "Peng-Jen Chen",
                "Changhan Wang",
                "Jiatao Gu",
                "Sravya Popuri",
                "Xutai Ma",
                "Adam Polyak",
                "Yossi Adi",
                "Qing He",
                "Yun Tang"
            ],
            "title": "Direct speech-to-speech translation with discrete units",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Ann Lee",
                "Hongyu Gong",
                "Paul-Ambroise Duquenne",
                "Holger Schwenk",
                "Peng-Jen Chen",
                "Changhan Wang",
                "Sravya Popuri",
                "Yossi Adi",
                "Juan Pino",
                "Jiatao Gu"
            ],
            "title": "Textless speech-to-speech translation on real data",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaoyu Liu",
                "Brian Mak"
            ],
            "title": "Cross-lingual multi-speaker text-to-speech synthesis for voice cloning without using parallel corpus for unseen speakers",
            "year": 1911
        },
        {
            "authors": [
                "Chutong Meng",
                "Junyi Ao",
                "Tom Ko",
                "Mingxuan Wang",
                "Haizhou Li"
            ],
            "title": "Cobert: Self-supervised speech representation learning through code representation learning",
            "venue": "In Interspeech",
            "year": 2023
        },
        {
            "authors": [
                "Satoshi Nakamura",
                "Konstantin Markov",
                "Hiromi Nakaiwa",
                "Gen-ichiro Kikui",
                "Hisashi Kawai",
                "Takatoshi Jitsuhiro",
                "Jinsong Zhang",
                "Hirofumi Yamamoto",
                "Eiichiro Sumita",
                "Seiichi Yamamoto"
            ],
            "title": "The ATR multilingual speech-to-speech translation system",
            "venue": "IEEE Trans. Speech Audio Process.,",
            "year": 2006
        },
        {
            "authors": [
                "Tu Anh Nguyen",
                "Beno\u0131\u0302t Sagot",
                "Emmanuel Dupoux"
            ],
            "title": "Are discrete units necessary for spoken language modeling",
            "venue": "IEEE J. Sel. Top. Signal Process.,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2015
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Kun Song",
                "Yi Ren",
                "Yi Lei",
                "Chunfeng Wang",
                "Kun Wei",
                "Lei Xie",
                "Xiang Yin",
                "Zejun Ma"
            ],
            "title": "Styles2st: Zero-shot style transfer for direct speech-to-speech translation",
            "venue": "arXiv preprint arXiv:2305.17732,",
            "year": 2023
        },
        {
            "authors": [
                "Andros Tjandra",
                "Sakriani Sakti",
                "Satoshi Nakamura"
            ],
            "title": "Speech-to-speech translation between untranscribed unknown languages. In IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019",
            "venue": "Singapore, December",
            "year": 2019
        },
        {
            "authors": [
                "Chengyi Wang",
                "Sanyuan Chen",
                "Yu Wu",
                "Ziqiang Zhang",
                "Long Zhou",
                "Shujie Liu",
                "Zhuo Chen",
                "Yanqing Liu",
                "Huaming Wang",
                "Jinyu Li"
            ],
            "title": "Neural codec language models are zero-shot text to speech synthesizers",
            "venue": "arXiv preprint arXiv:2301.02111,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mirjam Wester",
                "Hui Liang"
            ],
            "title": "The emime mandarin bilingual database",
            "venue": "Technical report, The University of Edinburgh,",
            "year": 2011
        },
        {
            "authors": [
                "Neil Zeghidour",
                "Alejandro Luebs",
                "Ahmed Omran",
                "Jan Skoglund",
                "Marco Tagliasacchi"
            ],
            "title": "Soundstream: An end-to-end neural audio codec",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Binbin Zhang",
                "Hang Lv",
                "Pengcheng Guo",
                "Qijie Shao",
                "Chao Yang",
                "Lei Xie",
                "Xin Xu",
                "Hui Bu",
                "Xiaoyu Chen",
                "Chenchen Zeng",
                "Di Wu",
                "Zhendong Peng"
            ],
            "title": "WENETSPEECH: A 10000+ hours multidomain mandarin corpus for speech recognition",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zhang",
                "Xu Tan",
                "Yi Ren",
                "Tao Qin",
                "Kejun Zhang",
                "Tie-Yan Liu"
            ],
            "title": "Uwspeech: Speech to speech translation for unwritten languages",
            "venue": "In Thirty-Fifth AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Dong Zhang",
                "Rong Ye",
                "Tom Ko",
                "Wang Mingxuan",
                "Zhou Yaqian"
            ],
            "title": "Dub: Discrete unit backtranslation for speech translation",
            "venue": "In Findings in ACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Ziqiang Zhang",
                "Long Zhou",
                "Chengyi Wang",
                "Sanyuan Chen",
                "Yu Wu",
                "Shujie Liu",
                "Zhuo Chen",
                "Yanqing Liu",
                "Huaming Wang",
                "Jinyu Li"
            ],
            "title": "Speak foreign languages with your own voice: Cross-lingual neural codec language modeling",
            "venue": "arXiv preprint arXiv:2303.03926,",
            "year": 2023
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong"
            ],
            "title": "A survey of large language models",
            "venue": "arXiv preprint arXiv:2303.18223,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Speech-to-speech translation (S2ST) is a challenging task as it encounters all the difficulties of automatic speech recognition (ASR), machine translation (MT) and text-to-speech (TTS) synthesis. The research in S2ST focuses on two approaches: cascade solutions (Lavie et al., 1997; Baldridge, 2004; Nakamura et al., 2006) and direct solutions (Jia et al., 2019; 2022a). The advantage of cascade solutions lies in the convenience of improving the performance of individual modules, while direct solutions excel in lower latency and simpler model architectures. As for the direct S2ST solutions, it used to involve direct output of mel-spectrogram features (Dong et al., 2022) in the early stages, but recently, there has been a growing interest in predicting discrete units (Lee et al., 2022a). The unit-based approach has become more popular due to several reasons: (1) It eases the modeling difficulty of emitting spectrogram. (2) Units can be generated through unsupervised methods and can cover unwritten languages. (3) It allows a connection with other token-based NLP models.\nRecently, language modeling (LM) approaches have made a lot of breakthroughs in natural language processing (NLP) (Zhao et al., 2023). The success of GPT models (Brown et al., 2020; Ouyang et al., 2022) is leading the community to a new era. Currently, the encoder-decoder models remain widely used in speech tasks, and the exploration of using LM approaches is still in its early stages. In fact, there have been attempts on ASR (Fathullah et al., 2023) and TTS (Wang et al., 2023), indicating that this direction is promising. Thus, we are motivated to investigate the performance of language modeling approach in S2ST. In this paper, we propose a semantic unit-based framework for S2ST system. Our framework (Fig. 1) consists of three LMs: a translation LM, a duration LM and a speech synthesis LM. The translation LM processes the semantic units of the source language and translates them into semantic units of the target language. The duration LM predicts the duration information of the target semantic units and extends the unit sequence. The speech synthesis LM predicts the target acoustic units which are then converted into a waveform by a unit vocoder.\nWe employ various prompt types to extract the acquired knowledge from the language models utilized in our approach. Specifically, we concatenate the source and target semantic units, along with the source acoustic units, forming a comprehensive prompt for the speech synthesis language model.\nThis enables the speech synthesis language model to grasp the acoustic characteristics of the source speaker and generate voice-cloned acoustic units accordingly. Importantly, both the semantic and acoustic units mentioned above are generated through unsupervised methods, making our framework applicable to unwritten languages.\nWe evaluate our system on Chinese \u2192 English and English \u2192 Spanish language pairs. Experimental results show that our system can generate voice-cloned speech with high translation quality and audio quality. We summarise our contribution as follows:\n\u2022 We propose using a series of decoder-only language models to fulfill the S2ST task, whereas encoder-decoder models are the dominant structure in previous works.\n\u2022 Unsupervised speech units are used in the framework and thus PolyVoice can cover both written and unwritten languages.\nThe rest of this paper is organized as follows. Section 2 introduces related work. Details of our method are described in Section 3. Section 4 introduces our experimental setup and main results. Section 5 presents our ablation study. Finally, we conclude our work in the last section."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 SPEECH TOKENIZATION",
            "text": "There are two kinds of discretized speech units used in our work: semantic and acoustic units. Semantic units are usually derived from representations produced by speech encoder models like HuBERT (Hsu et al., 2021), mHuBERT (Lee et al., 2022c) or w2v-BERT (Chung et al., 2021). They capture the phonetics and semantic content in speech. Although the making of these units is originally developed to be used as target for training the speech encoder, recently there are attempts to directly use these units as input/output for semantic tasks (Meng et al., 2023; Zhang et al., 2023a). Acoustic units can also be referred to as codec units. They are originally developed to transmit highquality speech signals under limited bandwidth. AudioLM (Borsos et al., 2023) is a pioneer work in using language models (LM) for audio generation. They make use of both kinds of units and build several LMs with different resolutions. VALL-E (Wang et al., 2023) further extends the AudioLM framework and applies it in TTS. They successfully demonstrate that the in-context learning capabilities of LM can be similarly replicated in the context of phoneme and codec units. In contrast to phoneme units which have to involve a supervised training process, both semantic and acoustic units can be generated through unsupervised methods."
        },
        {
            "heading": "2.2 TTS",
            "text": "Zero-shot cross-lingual TTS (Jia et al., 2018; Cooper et al., 2020) aims to build a system that can synthesize speech with user\u2019s voice and in a specific language that the user doesn\u2019t speak. Early attempts include speaker adaptation (Chen et al., 2019) and speaker embedding (Liu & Mak, 2019) approaches. LM-based TTS has been recently proposed and demonstrated promising results. VALLE (Wang et al., 2023) introduces an LM-based approach that leverages in-context learning for zeroshot TTS. Their approach utilizes phoneme units and source acoustic units to prompt the LM in predicting the target acoustic units. The most relevant related work to ours is VALL-E X (Zhang et al., 2023b), which extends the VALL-E framework to tackle the cross-lingual problem. In their work, they concatenate the source and target phoneme units, along with the source acoustic units, to create a prompt for the LM."
        },
        {
            "heading": "2.3 S2ST",
            "text": "Speech-to-speech translation (Lavie et al., 1997; Baldridge, 2004; Nakamura et al., 2006) aims to develop models capable of generating target language speech from source language speech. A vanilla system traditionally employs a pipeline (Nakamura et al., 2006) that sequentially processes the input through automatic speech recognition (ASR) models, machine translation (MT) models, and textto-speech synthesis (TTS) models. Recently, end-to-end paradigms (Jia et al., 2019; 2022a) have gained popularity in the field of S2ST, as they allow for a single model to perform one or more of\nthe aforementioned tasks, which consequently reduces error propagation and latency. Among the various techniques, auxiliary supervision based on textual data has been particularly effective during training (Jia et al., 2019; Kano et al., 2021). However, this approach is not feasible when dealing with unwritten languages. To address this challenge, discrete units (Hsu et al., 2021) extracted from the speech are used to replace the target text, and then can be synthesized into the speech (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022a). Large scale studies have shown the powerful performance in various speech processing tasks (Nguyen et al., 2022).\nCurrent research in speech-to-speech translation primarily emphasizes translation quality, with notable improvements observed in automatic evaluation metrics (like BLEU) or human evaluation of naturalness. However, there remain two persistent challenges in developing practical systems. First, these systems are predominantly developed and evaluated on small-scale benchmarks, while real-world scenarios often involve large quantities of labeled data, including ASR, MT, and S2T data (Agrawal et al., 2023). Even for low-resource or unwritten languages, leveraging unlabeled speech or text can provide valuable information (Lee et al., 2022a). Therefore, developing a unified model that jointly utilizes various data types is a critical research goal yet to be achieved. Second, while not a strict requirement, preserving the source speaker\u2019s style during translation is an important aspect of improving user experience (Zhang et al., 2023b; Song et al., 2023). However, capturing the unique characteristics of individual speakers is a challenging task. Current approaches, such as speaker embeddings (Jia et al., 2019) and multi-speaker TTS systems (Jia et al., 2018), have made some progress in this direction, but they are still far from practical requirements.\nTaking a broad perspective, our work aligns with the framework presented in Lee et al. (2022a), where the source speech undergoes translation into discrete units before synthesizing it into the target language\u2019s speech. However, what distinguishes our work is the utilization of decoder-only language models to enhance the performance of each module. By leveraging diverse data sources within a language model-based framework, our proposed method effectively maintains the source speaker\u2019s style during synthesis, thereby demonstrating significant potential in practical systems."
        },
        {
            "heading": "3 METHOD",
            "text": "We present PolyVoice, an innovative language model-based framework for speech-to-speech translation, catering to both written and unwritten languages. Our proposed framework leverages discrete units, acquired through self-supervised training techniques such as HuBERT (Hsu et al., 2021), serving as an intermediary representation bridging the source and target speech modalities. PolyVoice provides a comprehensive framework consisting of two main components: a speech-to-unit translation (S2UT) front-end, facilitating the conversion of source language speech into target language units, and a unit-to-speech (U2S) back-end, skillfully synthesizing translated speech while preserving the personalized style of the source speaker. Figure 1 provides an illustrative overview of our approach."
        },
        {
            "heading": "3.1 SPEECH-TO-UNIT TRANSLATION (S2UT)",
            "text": "By employing discrete units obtained through self-supervised training, semantically irrelevant information from continuous speech representations is eliminated, facilitating effective training in an NLP paradigm. In this regard, the S2UT component leverages a language model to acquire the necessary cross-lingual generation capabilities based on the unit-based approach.\nSemantic unit extractor S2UT initiates the processing of raw speech data by employing a sophisticated semantic unit extractor. Here we adopt HuBERT, which first encodes the speech by a stack of convolutions and Transformer layers to continuous representations at every 20-ms frame, and then utilizes k-means clustering to discretize the representation to a set of cluster indices Z = z 1, \u00b7 \u00b7 \u00b7 , z T . T is the number of frames and z t \u2208 [K], where K is the number of cluster centroids. The discretized units are then merged by removing consecutive duplicated units.\nUnit-based cross-lingual language model (U-XLM) We denote the training sample consisting of units of speech in source language and target language as <src unit, tgt unit>. Within the encoderdecoder architecture, the encoder takes the source units as input, while the decoder generates predictions for the target units. To facilitate the generation of cross-lingual units, a straightforward ap-\nproach involves utilizing simple prompts to construct training samples for natural language from unit pairs. For instance, one can create prompts like: \u201cTranslate [src lang] unit {src unit} to [tgt lang] unit: {tgt unit}\u201d. In addition to the direct transformation prompt, we can also instruct the model to generate intermediate steps similar to the cascaded systems in the chain of thought prompting (Wei et al., 2022) manner.\nTraining To achieve competitive performance in training the U-XLM model, a large amount of data is crucial. However, obtaining supervised data, specifically cross-lingual unit pairs, is often limited in real-world scenarios. While auxiliary models can generate pseudo labels, such as synthesizing the target speech using the TTS model, direct training with supervised data is preferred.\nTo overcome the challenge of limited data availability, prior research has incorporated additional loss functions into the encoder-decoder architecture using multitask learning (Jia et al., 2022a; Lee et al., 2022a). In our work, we leverage the power of language modeling to address this issue in a more straightforward manner, allowing for the utilization of diverse data sources like automatic speech recognition (ASR) and machine translation (MT) data.\nIn Table 1, we demonstrate how we slightly modify the prompts to create training samples for different types of data sources. By employing parameter sharing and simplifying the design of auxiliary objectives, we train the model using these modified prompts. This approach also enables the direct utilization of unlabeled text and speech data. Consequently, the model implicitly enhances the alignment of the representation space between speech units and text."
        },
        {
            "heading": "3.2 UNIT-TO-SPEECH SYNTHESIS (U2S)",
            "text": "Unit-to-speech language model (U-SLM) As illustrated in Figure 1, the U-SLM leverages the semantic units predicted by U-XLM and generates the codec units which incorporate the speaking style of source speaker. Similar to VALL-E X, U-SLM encompasses both an autoregressive model and a non-autoregressive model. However, instead of conventional phonemes, we employ discretized semantic units in our approach.\nSoundStream codec We employ SoundStream (Zeghidour et al., 2021), a neural audio codec, to generate embeddings of acoustic tokens. To ensure optimal performance, we re-implement the SoundStream with a hierarchy of 6 vector quantizers and a vocabulary of 1024 symbols. In our configuration, the acoustic tokens are produced at a rate of 80Hz for input waveforms sampled at 24 kHz, which results in a significant reduction in the sampling rate, specifically a reduction by a factor of 300 (24000/80). Once the U2S model predicts the acoustic tokens represented by the SoundStream codec, the decoder component of SoundStream reconstructs them back into the waveform.\nDuration model Through empirical observations, we have determined that the duration information of discretized units is crucial for ensuring stable and natural-sounding synthesized speech. In our approach, we employ an additional LM to predict the durations.\nIn Figure 1, we illustrate the process of incorporating duration prediction into our framework. The merged source semantic unit sequence, merged target semantic unit sequence, and the source duration value sequence (D) are concatenated and provided as a prompt to the duration LM. Subsequently, the duration LM predicts the target duration value sequence, and each target semantic unit is repeated accordingly based on its predicted duration."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We conduct experiments utilizing a decoder-only model architecture following the standard GPT2 (Radford et al., 2019). In Appendix A.2, we provide a comprehensive description of the model configurations employed in our work."
        },
        {
            "heading": "4.1 DATASETS AND PREPROCESSING",
            "text": ""
        },
        {
            "heading": "4.1.1 S2UT",
            "text": "Semantic tokens U-XLM is trained by cross-lingual unit data, which is extracted from the audio by HuBERT (Hsu et al., 2021) models. For Chinese audio, we utilize an open-source model based on WenetSpeech Chinese speech1. For English and Spanish audio, we use an open-source multilingual model (English, Spanish and French)2. The cluster centroids of a k-mean algorithm for the two models are 500 and 1,000, respectively.\nVocabulary To address the out-of-vocabulary problem and enable parameter sharing across languages, we utilize byte-level subword units3 that decompose each character into byte-sized pieces and achieve a final vocabulary size of 56,407, including 1,500 cluster centroids (<zh-0>,...<zh499> and <m-0>,...,<m-999>).\nDatasets Considering that the paired speech-to-speech (S2S) data is scarce, we synthesize the pseudo data from the ASR data utilizing in-house MT and TTS systems. In addition, various types of data resources provide better learning of the U-XLM model, like large-scale ASR and MT data. A more elaborate description of the used datasets can be found in Appendix Table 7.\nThe S2S data is sourced from WenetSpeech (Zhang et al., 2022) and GigaSpeech (Chen et al., 2021), respectively, maked as \u201cGigaS2S\u201d and \u201cWenetS2S\u201d. WenetSpeech is a Chinese ASR dataset with over 10,000 hours of speech data collected from YouTube. And we utilize a subset of 10,000 hours of GigaSpeech (Chen et al., 2021), an English ASR dataset collected from audiobooks, podcasts, and YouTube.\nThen we scale up the training data using specific prompts for various types of datasets. We utilize the LibriLight (Kahn et al., 2020) and the in-house ASR datasets. LibriLight is an unlabeled English speech dataset containing about 60,000 hours of speech. Since LibriLight has many long audios, we segment and recognize the audio based on the method of voice active detection (VAD) and in-house ASR system, generating the audio length ranging from 0.5 to 25s, and the average length is 7s. The in-house ASR dataset is a Chinese ASR dataset with 60,000 hours of speech. We also use the in-house Chinese-English MT dataset consisting of 44M sentence pairs."
        },
        {
            "heading": "4.1.2 U2S",
            "text": "The U-SLM is trained on the large open-source bilingual speech data, including WenetSpeech (Zhang et al., 2022) and LibriLight (Kahn et al., 2020). The Librilight is handled in the same way as U-XLM. WenetSpeech keeps the original data length unchanged. The duration of audio samples ranges from 0.5 to 20s, and the average duration is 2.5s. To further improve the synthesized quality, we use an additional 250-hour internal Chinese TTS data and 400-hour internal English TTS data."
        },
        {
            "heading": "4.2 EVALUATION",
            "text": "Our method is evaluated on two speech-to-speech benchmark datasets, EMIME (Wester & Liang, 2011) (Chinese \u2192 English) and CVSS (Jia et al., 2022b) (English \u2192 Spanish). Apart from the overall result, we report the separate performance on the S2UT front-end and U2S back-end. EMIME contains bilingual Chinese-English speech recorded by the same speakers. For CVSS, the translation speech is in voices automatically transferred from the corresponding source speech. To measure the performance of our system, we evaluate both the translation quality and the speech quality.\nTranslation Quality Following the previous setups, we recognize the speech output by an inhouse ASR system to compute BLEU scores (ASR-BLEU) for S2ST results using sacrebleu4.\n1https://github.com/TencentGameMate/chinese_speech_pretrain 2https://github.com/facebookresearch/fairseq/blob/main/examples/speech_\nto_speech/docs/textless_s2st_real_data.md 3https://github.com/huggingface/tokenizers 4https://github.com/mjpost/sacrebleu\nSpeech Quality The speech quality is evaluated by multiple metrics. The capability of voice clone is measured by the speaker similarity (ASV-Score), which is calculated by an ASV model5 to determine whether the synthesized speech is from the same speaker as the ground-truth speech. The naturalness of the speech output is evaluated by the automatic metric using NISQA6. And the pronunciation accuracy is evaluated using WER scores (ASR-WER) with an ASR model based on hubert-large7."
        },
        {
            "heading": "4.3 RESULTS AND ANALYSIS",
            "text": ""
        },
        {
            "heading": "4.3.1 S2ST RESULTS",
            "text": "Table 2 summarizes the overall performance of our method for S2ST. We conduct experiments on the EMIME dataset to enable direct comparisons with the most similar work VALL-E X. The cascade system treats S2ST as a pipeline of running an ASR model, an MT model, and a multi-speaker YourTTS model separately and sequentially. During the synthesis process, speaker information is integrated using speaker embeddings.\nWe first evaluate the capability to preserve the voice of the source speaker in the output speech, using the ASV score. We calculate speaker similarity between the source speech, target speech, and synthesized speech. We can run the U-XLM alone, where speech is synthesized by a Unit-based vocoder8 (Lee et al., 2022c). Due to the lack of explicit modeling of speaker characteristics, it produces particularly low ASV scores. Both the VALL-E X and PolyVoice systems, which adopt in-context learning, show superior performance over the speaker embedding based method. Notably, our method demonstrates better voice cloning capabilities when ground-truth target information is available.\nPolyVoice achieves a slightly enhanced translation quality (ASR-BLEU) but a remarkable improvement in speech quality (naturalness) compared with VALL-E X. When taking the ground-truth target information as input, PolyVoice is inferior to VALL-E X with a large gap of about 10 BLEU points, while the naturalness improves significantly. The semantic units are extracted from the speech by unsupervised learning, which inevitably introduces errors. Although units are considered \u201csemantic\u201d tokens, they still preserve some acoustic information. Therefore, unit-based modeling leads to better speech quality but worse translation quality. In contrast, phonemes obtained from the text ensure semantic correctness but lose the acoustic information. And future work can focus on enhancing the extraction of semantic information to improve translation quality.\n5https://github.com/Sanyuan-Chen/UniSpeech/tree/t-schen/asv_eval/ downstreams/speaker_verification#example-2\n6https://github.com/gabrielmittag/NISQA 7https://huggingface.co/facebook/hubert-large-ls960-ft 8https://github.com/facebookresearch/fairseq/blob/main/examples/speech_\nto_speech/docs/textless_s2st_real_data.md\nInterestingly, PolyVoice achieves better naturalness using the predicted units. We speculate that this is due to the language model\u2019s output having better fluency. U-XLM learns the speech distribution over the large scale of unit data, and tends to generate more natural sequences of units. However, this may interfere with the accuracy of the translation. We will explore this issue in the future."
        },
        {
            "heading": "4.3.2 UNWRITTEN LANGUAGE SCENARIO",
            "text": "We examine our proposed framework in the case where the source is a written language and the target is an unwritten language. In our setup, we train and evaluate an English\u2192Spanish S2ST system without the use of any Spanish text transcript. Table 3 summarizes the results. The ASR-BLEU (18.3) indicates that the Spanish speech generated by our system is semantically understandable. This demonstrates the ability of our S2ST system for the unwritten languages."
        },
        {
            "heading": "5 ABLATION STUDY",
            "text": ""
        },
        {
            "heading": "5.1 DECODER-ONLY VS. ENCODER-DECODER",
            "text": "Empirical studies in the field of natural language processing have revealed that the full potential of the decoder-only approach can be realized through the use of large model sizes and expansive datasets. As pioneers in exploring the application of language models to S2ST, we present a fair comparison of the two architectures, decoder-only and encoder-decoder, in Table 4. Two frameworks are trained with the same training data and similar parameters, approximately 0.6B in size. Interestingly, the decoder-only model yields a remarkable improvement of 3.9 BLEU points over the encoder-decoder counterpart9 (Lee et al., 2022b). When we synthesize the speech by U2S instead of vocoder, the performance gap is reduced, highlighting the robustness of our U2S back-end.\n9The encoder-decoder architecture is experimented using the implementation: https://github.com/ facebookresearch/fairseq/blob/main/examples/speech_to_speech/docs/direct_ s2st_discrete_units.md."
        },
        {
            "heading": "5.2 MULTI-TASK TRAINING",
            "text": "As discussed in Section 3, the language modeling enables direct training over the diverse data sources utilizing specific prompts. In this way, we combine additional large-scale ASR and MT data to fully explore the potential of our method. As shown in Table 5, U-XLM achieves promising performance for multiple tasks involved (including S2ST, ASR, ST, MT, and TTS) under the expanded data setting, which verifies the capability of the general modeling in the decoder-only architecture. In the traditional paradigm, we need to design a complex manner to combine multi-task learning, but language modeling only modifies the prompt to construct the training data."
        },
        {
            "heading": "5.3 ZERO-SHOT CROSS-LINGUAL UNIT-TO-SPEECH",
            "text": "We select samples with a duration between 4 and 10 seconds from LibriSpeech (Panayotov et al., 2015) dev-clean set to evaluate the zero-shot cross-lingual unit-to-speech module. And we randomly choose one audio from EMIME as the Chinese speech prompt.\nTable 6 shows the resynthesis performance of different speech synthesizers. Our TTS obtains better performance in both ASV and naturalness. We attribute the increase of WER to the difference in the amount of semantic information carried by phonemes and unsupervised units. This is consistent with the observation reported in the work of mHuBERT and AudioLM. If we remove the duration model from the U2S, the WER increases dramatically. Our guess is that the unit itself does not contain as much duration information as the phonemes. Therefore the duration model is essential when using unsupervised units.\nWe further train our own multilingual HuBERT model (mHuBERT zh en) with a combination of Chinese and English data. The model size is the same as the HuBERT-large model in (Hsu et al., 2021). We have observed a substantial reduction in the WER metric when utilizing the semantic units generated from mHuBERT zh en. Thus, we believe that a multilingual universal representation model trained with more parameters and data can generate better semantic units. We do not use mHuBERT zh en in our S2ST experiment because we need the mHuBERT (Lee et al., 2022c) to run the English\u2192Spanish experiment. The benefit of using mHuBERT zh en to the overall S2ST is left for future work."
        },
        {
            "heading": "6 CONCLUSION AND FUTURE WORK",
            "text": "This paper presents a new framework for speech-to-speech translation (S2ST) based on semantic units. The framework consists of three LMs: a translation LM, a duration LM and a speech synthesis LM. Through comprehensive experimentation, we provide evidence that our unit-based S2ST system surpasses existing systems in terms of ASR-BLEU, ASV, and naturalness metrics. Importantly, our system demonstrates its effectiveness in scenarios involving unwritten languages, where there is a lack of Spanish text transcripts for reference.\nGiven the significant impact of semantic unit quality on our system\u2019s performance, future research will focus on improving the generation of a more refined set of discrete units. We aim to explore techniques and methodologies that can contribute to enhancing the quality and diversity of the generated semantic units. Additionally, we plan to investigate how the system\u2019s performance can be further enhanced by scaling up parameters and expanding the available training data. By exploring the effects of increased model capacity and larger datasets, we anticipate uncovering potential improvements in system accuracy and overall effectiveness."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 DATASETS\nWe use a set of several datasets to train U-XLM model: GigaS2S, WenetS2S, LibriLight and some in-house ASR, MT datasets. Table 7 shows the detailed descriptions and statistics.\nA.2 MODEL SETTINGS\nA.2.1 S2UT\nIn the S2UT front-end, U-XLM\u2019s model architecture is a unidirectional Transformer decoder consisting of 48 layers with hidden size 1600, feed-forward network (FFN) size 6400, and 25 attention heads. The total parameters are 1.6B. U-XLM is trained on 8/32 NVIDIA TESLA A100 80GB GPUs with a batch size of 3072 tokens per GPU for 500k steps.\nA.2.2 U2S\nIn the U2S back-end, the U-SLM consists of 12 transformer layers. Each of these layers comprises 16 attention heads, an attention dimension of 1024, and an FFN dimension of 4096 in both the autoregressive (AR) model and non-autoregressive (NAR) model. We train the models using 8 NVIDIA TESLA A100 80GB GPUs, with a batch size of 8 utterances per GPU for 800k steps. Training for all steps takes about 5 days."
        }
    ],
    "year": 2023
}